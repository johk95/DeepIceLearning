
 ############################################
You are running the network script with arguments: 
virtual_len: -1
testing: False
filesizes: False
project: charge_h012_v1
crtfolders: False
continue: None
using: charge
date: 2017-08-23
input: h012
model: CNN_v1.cfg
############################################
 
[(0, 124352), (0, 125784), (0, 125878)]
[(124352, 165803), (125784, 167712), (125878, 167837)]
[(165803, 207253), (167712, 209639), (167837, 209796)]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv3d_1 (Conv3D)            (None, 9, 5, 28, 64)      3136      
_________________________________________________________________
batch_normalization_1 (Batch (None, 9, 5, 28, 64)      256       
_________________________________________________________________
activation_1 (Activation)    (None, 9, 5, 28, 64)      0         
_________________________________________________________________
conv3d_2 (Conv3D)            (None, 5, 3, 14, 64)      110656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 5, 3, 14, 64)      256       
_________________________________________________________________
activation_2 (Activation)    (None, 5, 3, 14, 64)      0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 13440)             0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 13440)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               6881792   
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 256)               131328    
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 257       
=================================================================
Total params: 7,127,681
Trainable params: 7,127,425
Non-trainable params: 256
_________________________________________________________________
None
Epoch 1/30
here
1.90484968478
exiting
here
1.90484968478
exiting
Epoch 00000: val_loss improved from inf to 0.47444, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.15 GB
861s - loss: 0.5231 - acc: 0.7989 - val_loss: 0.4744 - val_acc: 0.8109
Epoch 2/30
Epoch 00001: val_loss did not improve
RAM Usage 1.20 GB
831s - loss: 0.4711 - acc: 0.8061 - val_loss: 0.4962 - val_acc: 0.8139
Epoch 3/30
Epoch 00002: val_loss improved from 0.47444 to 0.46977, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
832s - loss: 0.4474 - acc: 0.8201 - val_loss: 0.4698 - val_acc: 0.8159
Epoch 4/30
Epoch 00003: val_loss improved from 0.46977 to 0.42844, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
820s - loss: 0.4326 - acc: 0.8269 - val_loss: 0.4284 - val_acc: 0.8348
Epoch 5/30
Epoch 00004: val_loss improved from 0.42844 to 0.41913, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
818s - loss: 0.4046 - acc: 0.8405 - val_loss: 0.4191 - val_acc: 0.8412
Epoch 6/30
Epoch 00005: val_loss improved from 0.41913 to 0.40150, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
786s - loss: 0.3953 - acc: 0.8438 - val_loss: 0.4015 - val_acc: 0.8415
Epoch 7/30
Epoch 00006: val_loss improved from 0.40150 to 0.39097, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
754s - loss: 0.3799 - acc: 0.8480 - val_loss: 0.3910 - val_acc: 0.8460
Epoch 8/30
Epoch 00007: val_loss improved from 0.39097 to 0.37855, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
741s - loss: 0.3691 - acc: 0.8501 - val_loss: 0.3786 - val_acc: 0.8479
Epoch 9/30
Epoch 00008: val_loss improved from 0.37855 to 0.36235, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
739s - loss: 0.3614 - acc: 0.8511 - val_loss: 0.3624 - val_acc: 0.8498
Epoch 10/30
Epoch 00009: val_loss improved from 0.36235 to 0.35517, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
740s - loss: 0.3551 - acc: 0.8530 - val_loss: 0.3552 - val_acc: 0.8517
Epoch 11/30
Epoch 00010: val_loss improved from 0.35517 to 0.35196, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
739s - loss: 0.3499 - acc: 0.8540 - val_loss: 0.3520 - val_acc: 0.8512
Epoch 12/30
Epoch 00011: val_loss did not improve
RAM Usage 1.20 GB
740s - loss: 0.3458 - acc: 0.8548 - val_loss: 0.3570 - val_acc: 0.8537
Epoch 13/30
Epoch 00012: val_loss did not improve
RAM Usage 1.20 GB
738s - loss: 0.3420 - acc: 0.8554 - val_loss: 0.3596 - val_acc: 0.8511
Epoch 14/30
Epoch 00013: val_loss improved from 0.35196 to 0.34534, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
739s - loss: 0.3385 - acc: 0.8566 - val_loss: 0.3453 - val_acc: 0.8530
Epoch 15/30
Epoch 00014: val_loss did not improve
RAM Usage 1.20 GB
743s - loss: 0.3353 - acc: 0.8574 - val_loss: 0.3672 - val_acc: 0.8522
Epoch 16/30
Epoch 00015: val_loss did not improve
RAM Usage 1.20 GB
741s - loss: 0.3323 - acc: 0.8582 - val_loss: 0.3510 - val_acc: 0.8542
Epoch 17/30
Epoch 00016: val_loss did not improve
RAM Usage 1.20 GB
740s - loss: 0.3297 - acc: 0.8586 - val_loss: 0.3528 - val_acc: 0.8540
Epoch 18/30
Epoch 00017: val_loss improved from 0.34534 to 0.33958, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-23/charge_h012_v1/best_val_loss.npy
RAM Usage 1.20 GB
739s - loss: 0.3269 - acc: 0.8596 - val_loss: 0.3396 - val_acc: 0.8558
Epoch 19/30
Epoch 00018: val_loss did not improve
RAM Usage 1.20 GB
740s - loss: 0.3251 - acc: 0.8598 - val_loss: 0.3576 - val_acc: 0.8545
Epoch 20/30
Epoch 00019: val_loss did not improve
RAM Usage 1.20 GB
737s - loss: 0.3225 - acc: 0.8611 - val_loss: 0.3614 - val_acc: 0.8531
Epoch 21/30
Epoch 00020: val_loss did not improve
RAM Usage 1.20 GB
737s - loss: 0.3211 - acc: 0.8613 - val_loss: 0.3716 - val_acc: 0.8530
Epoch 22/30
Epoch 00021: val_loss did not improve
RAM Usage 1.20 GB
737s - loss: 0.3185 - acc: 0.8619 - val_loss: 0.3794 - val_acc: 0.8488
Epoch 23/30
Epoch 00022: val_loss did not improve
RAM Usage 1.20 GB
738s - loss: 0.3173 - acc: 0.8621 - val_loss: 0.3782 - val_acc: 0.8484
Epoch 24/30
Epoch 00023: val_loss did not improve
RAM Usage 1.20 GB
739s - loss: 0.3146 - acc: 0.8629 - val_loss: 0.3746 - val_acc: 0.8499
Epoch 25/30
Epoch 00024: val_loss did not improve
RAM Usage 1.20 GB
744s - loss: 0.3132 - acc: 0.8634 - val_loss: 0.4114 - val_acc: 0.8342
Epoch 00024: early stopping
time to fit: 5h 23min 9.21sec

 Save the Model 


 Calculate Results... 

Predict Values for 11069_00000-00999.h5
Predict Values for 11069_01000-01999.h5
Predict Values for 11069_02000-02999.h5
104406 / 125336 =  83.30%
 
 Finished .... 
