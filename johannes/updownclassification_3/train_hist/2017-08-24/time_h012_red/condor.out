
 ############################################
You are running the network script with arguments: 
virtual_len: -1
testing: False
filesizes: False
project: time_h012_red
crtfolders: False
continue: None
using: time
date: 2017-08-24
input: h012
model: CNN_red.cfg
############################################
 
[(0, 124352), (0, 125784), (0, 125878)]
[(124352, 165803), (125784, 167712), (125878, 167837)]
[(165803, 207253), (167712, 209639), (167837, 209796)]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv3d_1 (Conv3D)            (None, 9, 5, 28, 64)      3136      
_________________________________________________________________
batch_normalization_1 (Batch (None, 9, 5, 28, 64)      256       
_________________________________________________________________
activation_1 (Activation)    (None, 9, 5, 28, 64)      0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 80640)             0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 80640)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               10322048  
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 64)                8256      
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 65        
=================================================================
Total params: 10,333,761
Trainable params: 10,333,633
Non-trainable params: 128
_________________________________________________________________
None
Epoch 1/30
Epoch 00000: val_loss improved from inf to 0.44238, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-24/time_h012_red/best_val_loss.npy
RAM Usage 1.15 GB
854s - loss: 0.4807 - acc: 0.8006 - val_loss: 0.4424 - val_acc: 0.8291
Epoch 2/30
Epoch 00001: val_loss improved from 0.44238 to 0.41754, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-24/time_h012_red/best_val_loss.npy
RAM Usage 1.15 GB
825s - loss: 0.4215 - acc: 0.8277 - val_loss: 0.4175 - val_acc: 0.8351
Epoch 3/30
Epoch 00002: val_loss did not improve
RAM Usage 1.15 GB
826s - loss: 0.4072 - acc: 0.8322 - val_loss: 0.4322 - val_acc: 0.8382
Epoch 4/30
Epoch 00003: val_loss did not improve
RAM Usage 1.15 GB
826s - loss: 0.3865 - acc: 0.8390 - val_loss: 0.4182 - val_acc: 0.8347
Epoch 5/30
Epoch 00004: val_loss improved from 0.41754 to 0.40941, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-24/time_h012_red/best_val_loss.npy
RAM Usage 1.15 GB
827s - loss: 0.3768 - acc: 0.8417 - val_loss: 0.4094 - val_acc: 0.8316
Epoch 6/30
Epoch 00005: val_loss improved from 0.40941 to 0.39625, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-24/time_h012_red/best_val_loss.npy
RAM Usage 1.15 GB
828s - loss: 0.3686 - acc: 0.8456 - val_loss: 0.3963 - val_acc: 0.8320
Epoch 7/30
Epoch 00006: val_loss did not improve
RAM Usage 1.15 GB
826s - loss: 0.3653 - acc: 0.8465 - val_loss: 0.3969 - val_acc: 0.8341
Epoch 8/30
Epoch 00007: val_loss improved from 0.39625 to 0.38162, saving model to /data/user/jkager/NN_Reco/johannes/updownclassification_3/train_hist/2017-08-24/time_h012_red/best_val_loss.npy
RAM Usage 1.15 GB
828s - loss: 0.3486 - acc: 0.8521 - val_loss: 0.3816 - val_acc: 0.8397
Epoch 9/30
Epoch 00008: val_loss did not improve
RAM Usage 1.15 GB
826s - loss: 0.3530 - acc: 0.8511 - val_loss: 0.3916 - val_acc: 0.8375
Epoch 10/30
Epoch 00009: val_loss did not improve
RAM Usage 1.15 GB
827s - loss: 0.3382 - acc: 0.8570 - val_loss: 0.3833 - val_acc: 0.8348
Epoch 11/30
Epoch 00010: val_loss did not improve
RAM Usage 1.15 GB
826s - loss: 0.3362 - acc: 0.8585 - val_loss: 0.3875 - val_acc: 0.8392
Epoch 12/30
Epoch 00011: val_loss did not improve
RAM Usage 1.15 GB
825s - loss: 0.3271 - acc: 0.8625 - val_loss: 0.3965 - val_acc: 0.8309
Epoch 13/30
Epoch 00012: val_loss did not improve
RAM Usage 1.15 GB
826s - loss: 0.3201 - acc: 0.8660 - val_loss: 0.3852 - val_acc: 0.8357
Epoch 14/30
Epoch 00013: val_loss did not improve
RAM Usage 1.15 GB
826s - loss: 0.3183 - acc: 0.8678 - val_loss: 0.3839 - val_acc: 0.8362
Epoch 15/30
Epoch 00014: val_loss did not improve
RAM Usage 1.15 GB
829s - loss: 0.3075 - acc: 0.8721 - val_loss: 0.4115 - val_acc: 0.8294
Epoch 00014: early stopping
time to fit: 3h 27min 48.00sec

 Save the Model 


 Calculate Results... 

Predict Values for 11069_00000-00999.h5
Predict Values for 11069_01000-01999.h5
Predict Values for 11069_02000-02999.h5
103982 / 125336 =  82.96%
 
 Finished .... 
