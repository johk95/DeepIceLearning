Traceback (most recent call last):
  File "/mnt/lfs3/user/jkager/NN_Reco/johannes/updownclassification_3/condor_submit/../updown_network.py", line 316, in <module>
    max_q_size=int(parser.get('Training_Parameters', 'max_queue_size'))
  File "/home/jkager/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/jkager/.local/lib/python2.7/site-packages/keras/models.py", line 1124, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/jkager/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/jkager/.local/lib/python2.7/site-packages/keras/engine/training.py", line 1902, in fit_generator
    class_weight=class_weight)
  File "/home/jkager/.local/lib/python2.7/site-packages/keras/engine/training.py", line 1642, in train_on_batch
    outputs = self.train_function(ins)
  File "/home/jkager/.local/lib/python2.7/site-packages/keras/backend/theano_backend.py", line 1196, in __call__
    return self.function(*inputs)
  File "/home/jkager/.local/lib/python2.7/site-packages/theano/compile/function_module.py", line 898, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/home/jkager/.local/lib/python2.7/site-packages/theano/gof/link.py", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/home/jkager/.local/lib/python2.7/site-packages/theano/compile/function_module.py", line 884, in __call__
    self.fn() if output_subset is None else\
ValueError: dimension mismatch in args to gemm (1000,1536)x(6720,512)->(1000,512)
Apply node that caused the error: GpuDot22(if{inplace,gpu}.0, dense_1/kernel)
Toposort index: 289
Inputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]
Inputs shapes: [(1000, 1536), (6720, 512)]
Inputs strides: [(1536, 1), (512, 1)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[GpuElemwise{Add}[(0, 0)](GpuDot22.0, GpuDimShuffle{x,0}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
